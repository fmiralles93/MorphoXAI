# -*- coding: utf-8 -*-
"""MorphoXAI_VF.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I0jvVH2IJne8VK7qCzT5Z2qOJkhCMICX
"""

# Instalaciones necesarias (solo una vez)
!pip install mediapipe opencv-python

# Imports est√°ndar
import os, cv2, random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, random_split, Subset
from torchvision import transforms, models

# Drive y dispositivo
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Funci√≥n para clasificar edades
def get_age_group(age):
    if age <= 20:
        return "Teen"
    elif age <= 40:
        return "Young adult"
    elif age <= 60:
        return "Middle-aged"
    else:
        return "Senior"

# Dataset personalizado
class UTKFaceDataset(Dataset):
    def __init__(self, folder_path, transform=None, max_images=None, min_age=11):
        self.folder_path = folder_path
        self.image_files = [f for f in os.listdir(folder_path) if f.endswith(".jpg")]
        if max_images:
            self.image_files = self.image_files[:max_images]
        self.transform = transform

        self.data = []
        for filename in self.image_files:
            try:
                age, gender, _ = filename.split("_")[:3]
                age = int(age)
                gender = int(gender)
                if age < min_age:
                    continue  # üßí Excluir menores de 10 a√±os
                age_group = get_age_group(age)
                self.data.append({
                    'filename': filename,
                    'age': age,
                    'gender': gender,
                    'age_group': age_group
                })
            except Exception as e:
                print(f"‚ùå Error parsing {filename}: {e}")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        entry = self.data[idx]
        img_path = os.path.join(self.folder_path, entry['filename'])
        image = Image.open(img_path).convert("RGB")
        if self.transform:
            image = self.transform(image)
        return image, entry['gender'], entry['age'], entry['age_group'], entry['filename']

from torchvision import transforms
from torch.utils.data import DataLoader, Subset
from collections import defaultdict
import random

# üé® Transformaciones para 224x224
transform_train = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
    transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
])

transform_test = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
])

# üìÇ Cargar dataset filtrado (excluye <=10 a√±os)
folder_path = "/content/drive/MyDrive/Colab Notebooks/MSC HEALTH DATA SCIENCE/Dissertation/UTKFace"
dataset_full = UTKFaceDataset(folder_path=folder_path, transform=None, max_images=30000, min_age=10)

# ‚úÖ Divisi√≥n estratificada por grupo de edad (con m√≠nimo de 500 Teen en test)
indices_por_grupo = defaultdict(list)
for idx, sample in enumerate(dataset_full):
    _, _, age, group_age, _ = sample
    indices_por_grupo[group_age].append(idx)

test_indices, train_indices = [], []
for grupo, indices in indices_por_grupo.items():
    random.seed(42)
    random.shuffle(indices)
    if grupo == "Teen":
        n_test = max(int(0.2 * len(indices)), 400)
    else:
        n_test = int(0.2 * len(indices))
    test_indices.extend(indices[:n_test])
    train_indices.extend(indices[n_test:])

# Crear Subset a partir de los √≠ndices balanceados
train_ds = Subset(dataset_full, train_indices)
test_ds = Subset(dataset_full, test_indices)

# Aplicar transforms por separado
train_ds.dataset.transform = transform_train
test_ds.dataset.transform = transform_test

# ‚ö° DataLoaders r√°pidos
train_loader = DataLoader(train_ds, batch_size=64, shuffle=True,
                          num_workers=4, pin_memory=True)

test_loader = DataLoader(test_ds, batch_size=64, shuffle=False,
                         num_workers=4, pin_memory=True)

print(f"‚úÖ Dataset estratificado cargado: {len(train_ds)} entrenamiento | {len(test_ds)} test")

from collections import Counter

# Contar por grupo de edad
conteo_grupos = Counter([sample[3] for sample in test_ds])
print("üßÆ Conteo por grupo de edad:")
for grupo, cantidad in conteo_grupos.items():
    print(f" - {grupo}: {cantidad} im√°genes")

# Acceder directamente a los Teens
print(f"\nüë¶ Total de im√°genes en el grupo 'Teen': {conteo_grupos['Teen']}")

from torchvision import models
import torch.nn as nn

def get_finetuned_vgg16_224():
    # üì• Cargar VGG16 con pesos preentrenados
    vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)

    # ‚ùÑÔ∏è Congelar todas las capas convolucionales
    for param in vgg.features.parameters():
        param.requires_grad = False

    # üß† Reemplazar el clasificador final
    vgg.classifier = nn.Sequential(
        nn.Linear(25088, 512),
        nn.ReLU(inplace=True),
        nn.Dropout(0.5),
        nn.Linear(512, 1),
        nn.Sigmoid()
    )

    return vgg.to("cuda")

from sklearn.metrics import accuracy_score, roc_auc_score
import torch

def train_model_vgg16_224(model, train_loader, test_loader, epochs=30, lr=1e-4):
    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)
    criterion = torch.nn.BCELoss()
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)

    best_acc = 0
    for epoch in range(epochs):
        model.train()
        running_loss = 0
        for batch_idx, batch in enumerate(train_loader):
            x = batch[0].float().to("cuda")
            y = batch[1].float().to("cuda")  # ‚úÖ Convertir a float
            optimizer.zero_grad()
            preds = model(x).squeeze()
            loss = criterion(preds, y)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

            if batch_idx % 40 == 0:
                print(f"üåÄ Epoch {epoch+1} | Batch {batch_idx}/{len(train_loader)} | Loss: {loss.item():.4f}")

        val_loss, acc, auc = evaluate_model_vgg16(model, test_loader, criterion)
        print(f"üìö Epoch {epoch+1}/{epochs} | Train Loss: {running_loss/len(train_loader):.4f} | Val Acc: {acc:.2f}% | AUC: {auc:.4f}")
        scheduler.step(val_loss)

        if acc > best_acc:
            best_acc = acc
            torch.save(model.state_dict(), "best_vgg16_224.pt")

    print(f"‚úÖ Entrenamiento finalizado. Mejor accuracy: {best_acc:.2f}%")

def evaluate_model_vgg16(model, loader, loss_fn):
    model.eval()
    all_probs, all_labels = [], []
    total_loss = 0

    with torch.no_grad():
        for batch in loader:
            x = batch[0].float().to("cuda")
            y = batch[1].float().to("cuda")  # ‚úÖ Convertir a float
            probs = model(x).squeeze()
            loss = loss_fn(probs, y)
            total_loss += loss.item()
            all_probs.extend(probs.cpu().numpy())
            all_labels.extend(y.cpu().numpy())

    preds_bin = [1 if p >= 0.5 else 0 for p in all_probs]
    acc = accuracy_score(all_labels, preds_bin) * 100
    auc = roc_auc_score(all_labels, all_probs)
    avg_loss = total_loss / len(loader)
    return avg_loss, acc, auc

model = get_finetuned_vgg16_224()
train_model_vgg16_224(model, train_loader, test_loader, epochs=30, lr=1e-4)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc
import matplotlib.pyplot as plt
import numpy as np
import torch

def plot_model_evaluation(model, loader):
    model.eval()
    all_probs, all_labels = [], []

    with torch.no_grad():
        for batch in loader:
            x = batch[0].float().to("cuda")
            y = batch[1].float().to("cuda")  # ‚úÖ Convertir expl√≠citamente a float
            probs = model(x).squeeze()
            all_probs.extend(probs.cpu().numpy())
            all_labels.extend(y.cpu().numpy())

    # üìå Convertir a clases binarias con umbral 0.5
    preds = [1 if p >= 0.5 else 0 for p in all_probs]

    # üî∑ Matriz de Confusi√≥n
    cm = confusion_matrix(all_labels, preds)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Male", "Female"])
    disp.plot(cmap=plt.cm.Blues, values_format="d")
    plt.title("Confusion Matrix (1=Female, 0=Male)")
    plt.grid(False)
    plt.show()

    # üî∑ Curva ROC
    fpr, tpr, _ = roc_curve(all_labels, all_probs)
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(6, 5))
    plt.plot(fpr, tpr, label=f"AUC = {roc_auc:.4f}", color='darkorange', linewidth=2)
    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC curve (gender classification)")
    plt.legend(loc="lower right")
    plt.grid(True)
    plt.tight_layout()
    plt.show()

plot_model_evaluation(model, test_loader)

import matplotlib.pyplot as plt
import numpy as np

def show_predictions(model, loader, n=8):
    model.eval()
    images_shown = 0
    fig = plt.figure(figsize=(20, 5))

    with torch.no_grad():
        for batch in loader:
            x = batch[0].to("cuda")
            y = batch[1].to("cuda")  # ‚úÖ Convertir a float si necesario, pero no afecta visualizaci√≥n
            preds = model(x).squeeze()
            preds_label = (preds > 0.5).long()
            for i in range(x.shape[0]):
                ax = fig.add_subplot(1, n, images_shown + 1)
                img = x[i].cpu().permute(1, 2, 0) * 0.5 + 0.5  # des-normalizar
                ax.imshow(img.numpy())
                pred = "Male" if preds_label[i] == 0 else "Female"
                true = "Male" if y[i] == 0 else "Female"
                color = "green" if pred == true else "red"
                ax.set_title(f"P: {pred}\nT: {true}", color=color)
                ax.axis("off")
                images_shown += 1
                if images_shown >= n:
                    plt.tight_layout()
                    plt.show()
                    return

show_predictions(model, test_loader, n=8)

# Grad-CAM
class GradCAM:
    def __init__(self, model, target_layer, device):
        self.model = model
        self.target_layer = target_layer
        self.device = device
        self.gradients = None
        self.activations = None
        self._register_hooks()

    def _register_hooks(self):
        for name, module in self.model.named_modules():
            if name == self.target_layer:
                module.register_forward_hook(self._forward_hook)
                module.register_backward_hook(self._backward_hook)

    def _forward_hook(self, module, input, output):
        self.activations = output.detach()

    def _backward_hook(self, module, grad_input, grad_output):
        self.gradients = grad_output[0].detach()

    def generate_cam(self, input_tensor):
        input_tensor = input_tensor.unsqueeze(0).to(self.device)
        input_tensor.requires_grad = True

        output = self.model(input_tensor)
        score = output.squeeze()
        if score.dim() > 0:
            score = score[0]

        self.model.zero_grad()
        score.backward()

        weights = self.gradients.mean(dim=[2, 3], keepdim=True)
        cam = (weights * self.activations).sum(dim=1).squeeze()
        cam = torch.relu(cam)
        cam = cam - cam.min()
        cam = cam / (cam.max() + 1e-8)
        cam_resized = cv2.resize(cam.cpu().numpy(), (224, 224))
        return cam_resized

# Score-CAM
class ScoreCAM:
    def __init__(self, model, target_layer, device):
        self.model = model
        self.target_layer = target_layer
        self.device = device
        self.activations = None
        self._register_hooks()

    def _register_hooks(self):
        for name, module in self.model.named_modules():
            if name == self.target_layer:
                module.register_forward_hook(self._hook_fn)

    def _hook_fn(self, module, input, output):
        self.activations = output.detach()

    def generate_cam(self, input_tensor):
        input_tensor = input_tensor.unsqueeze(0).to(self.device)
        _ = self.model(input_tensor)
        activations = self.activations.squeeze(0)
        weights = []
        upsampled = []

        for i in range(activations.shape[0]):
            fmap = activations[i].cpu().numpy()
            fmap_norm = (fmap - fmap.min()) / (fmap.max() - fmap.min() + 1e-8)
            fmap_resized = cv2.resize(fmap_norm, (224, 224))
            mask = torch.from_numpy(fmap_resized).unsqueeze(0).unsqueeze(0).to(self.device)
            masked_img = input_tensor * mask

            with torch.no_grad():
                score = self.model(masked_img).squeeze()
                if score.dim() > 0:
                    score = score[0]
                score = score.item()

            weights.append(score)
            upsampled.append(fmap_resized)

        weights = np.array(weights)
        upsampled = np.stack(upsampled)
        cam = np.sum(weights[:, None, None] * upsampled, axis=0)
        cam = np.maximum(cam, 0)
        cam -= cam.min()
        cam /= cam.max() + 1e-8
        return cam

vgg_gender = model  # o el nombre de tu modelo entrenado
device = torch.device("cuda")

gradcam = GradCAM(model=vgg_gender, target_layer="features.28", device=device)
scorecam = ScoreCAM(model=vgg_gender, target_layer="features.28", device=device)

import matplotlib.pyplot as plt
import numpy as np
import cv2
import torch

def visualize_cam_on_image(dataset, model, gradcam, scorecam, idx=0):
    model.eval()
    image, gender, _, _, filename = dataset[idx]
    input_tensor = image.to(device)

    # üéØ Obtener mapas
    gradcam_map = gradcam.generate_cam(input_tensor)
    scorecam_map = scorecam.generate_cam(input_tensor)

    # üñºÔ∏è Convertir imagen original para visualizaci√≥n
    img_np = image.permute(1, 2, 0).cpu().numpy()
    img_np = (img_np * 0.5 + 0.5) * 255  # desnormalizar
    img_np = np.clip(img_np, 0, 255).astype(np.uint8)

    # üî• Superponer heatmaps
    def overlay_cam(cam, img):
        heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)
        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)
        overlay = heatmap * 0.4 + img
        return np.uint8(np.clip(overlay, 0, 255))

    overlay_grad = overlay_cam(gradcam_map, img_np)
    overlay_score = overlay_cam(scorecam_map, img_np)

    # üñºÔ∏è Mostrar resultados
    plt.figure(figsize=(15, 5))
    plt.subplot(1, 3, 1)
    plt.imshow(img_np)
    plt.title(f"Original ({'Female' if gender==1 else 'Male'})")
    plt.axis('off')

    plt.subplot(1, 3, 2)
    plt.imshow(overlay_grad)
    plt.title("Grad-CAM")
    plt.axis('off')

    plt.subplot(1, 3, 3)
    plt.imshow(overlay_score)
    plt.title("Score-CAM")
    plt.axis('off')

    plt.tight_layout()
    plt.show()

visualize_cam_on_image(test_ds, vgg_gender, gradcam, scorecam, idx=0)

import mediapipe as mp

# Inicializar FaceMesh
mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True)

# Regiones anat√≥micas basadas en landmarks de MediaPipe
regiones = {
    "left_eye": [33, 160, 158, 133, 153, 144, 145],
    "right_eye": [362, 385, 387, 263, 373, 380, 374],
    "lips": [152, 148, 176, 149, 150, 18, 88, 199, 200, 201, 202, 329, 330, 331, 332, 333],
    "chin_center": [152, 148, 176, 149, 150],
    "chin_left": [136, 172, 58, 17],
    "chin_right": [365, 397, 288, 18],
    "left_cheek": [203, 204, 205, 50],
    "right_cheek": [425, 426, 291, 294, 278],
    "left_temple": [127, 234, 93],
    "right_temple": [356, 454, 323],
    "forehead": [10, 338, 297, 109, 67, 295, 282, 8],
    "left_cheekbone": [50, 101, 118, 205],
    "right_cheekbone": [291, 305, 334, 426],
    "nose": [1, 2, 4, 5, 6, 97, 327, 330, 168, 195],
    "left_eyebrow": [65, 70, 63, 105, 66],
    "right_eyebrow": [295, 336, 296, 334, 293],
    "left_jaw": [132, 137, 149, 234],
    "right_jaw": [361, 366, 288, 454]
}

# Sigma personalizado por regi√≥n para difuminar la m√°scara
sigmas_por_region = {
    "left_eye": 7, "right_eye": 7,
    "lips": 14,
    "chin_center": 11, "chin_left": 10, "chin_right": 10,
    "left_cheek": 13, "right_cheek": 13,
    "left_temple": 10, "right_temple": 10,
    "forehead": 20,
    "left_cheekbone": 10, "right_cheekbone": 10,
    "nose": 13,
    "left_eyebrow": 8, "right_eyebrow": 8,
    "left_jaw": 15, "right_jaw": 15
}

# Funci√≥n para crear m√°scara gaussiana centrada en (x, y)
def gaussian_mask(center, shape=(224, 224), sigma=10):
    x, y = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]))
    mask = np.exp(-((x - center[0])**2 + (y - center[1])**2) / (2 * sigma**2))
    return mask / mask.max()

# Detecci√≥n de landmarks y creaci√≥n de m√°scaras
def detectar_landmarks_y_mascaras(image_tensor):
    # De-normalizar imagen
    image_np = image_tensor.permute(1, 2, 0).cpu().numpy()
    image_np = (image_np * [0.229, 0.224, 0.225]) + [0.485, 0.456, 0.406]
    image_np = np.clip(image_np, 0, 1)
    image_rgb = (image_np * 255).astype(np.uint8)

    # Detecci√≥n con MediaPipe
    results = face_mesh.process(image_rgb)
    if not results.multi_face_landmarks:
        return None

    height, width, _ = image_rgb.shape
    landmarks = results.multi_face_landmarks[0].landmark
    puntos = [(int(lm.x * width), int(lm.y * height)) for lm in landmarks]

    # Crear m√°scaras por regi√≥n
    masks = {}
    for region, idxs in regiones.items():
        region_pts = [puntos[i] for i in idxs if i < len(puntos)]
        if not region_pts:
            continue
        x_mean = int(np.mean([p[0] for p in region_pts]))
        y_mean = int(np.mean([p[1] for p in region_pts]))
        sigma = sigmas_por_region.get(region, 10)
        masks[region] = gaussian_mask(center=(x_mean, y_mean), shape=(224, 224), sigma=sigma)

    return puntos, masks

# Cuantificar activaci√≥n CAM en cada regi√≥n anat√≥mica
def cuantificar_activacion_soft(cam_norm, masks):
    total = cam_norm.sum()
    activaciones = {}
    used_pixels = np.zeros_like(cam_norm, dtype=bool)

    for region, mask in masks.items():
        activacion = (cam_norm * mask).sum()
        activaciones[region] = float(activacion / total) if total > 0 else 0.0
        used_pixels |= (mask > 0.01)

    activaciones["other"] = float(cam_norm[~used_pixels].sum() / total) if total > 0 else 0.0
    return activaciones

# Crear mapa sem√°ntico a partir de activaciones
def generar_semantic_map(activaciones, masks):
    mapa = np.zeros((224, 224))
    for region, mask in masks.items():
        peso = activaciones.get(region, 0.0)
        mapa += peso * mask
    mapa = np.clip(mapa * 5, 0, 1)
    return mapa

# Procesar lote completo (exporta CSV)
def procesar_batch_semantic(val_loader, gradcam, scorecam, save_path, max_images=20000):
    registros = []
    count = 0

    for batch in val_loader:
        image, label, age, group_age, filename = batch
        image = image[0].to(device)
        label = int(label[0].item())
        age = int(age[0].item())
        group_age = group_age[0]
        filename = filename[0]

        resultado = detectar_landmarks_y_mascaras(image.cpu())
        if resultado is None:
            continue
        puntos, masks = resultado

        try:
            cam_grad = gradcam.generate_cam(image)
            cam_score = scorecam.generate_cam(image)

            act_grad = cuantificar_activacion_soft(cam_grad, masks)
            act_score = cuantificar_activacion_soft(cam_score, masks)

            fila = {
                "filename": filename,
                "label": label,
                "age": age,
                "age_group": group_age
            }

            for region in act_grad:
                fila[f"{region}_grad"] = act_grad[region]
                fila[f"{region}_score"] = act_score[region]

            registros.append(fila)
            count += 1
            if count >= max_images:
                break
        except Exception as e:
            print(f"‚ùå Error en imagen {filename}: {e}")
            continue

    df = pd.DataFrame(registros)
    df.to_csv(save_path, index=False)
    print(f"Activations stored in: {save_path}")

from collections import defaultdict
import random
from torch.utils.data import Subset, DataLoader

output_csv_balanceado = "/content/drive/MyDrive/Colab Notebooks/MSC HEALTH DATA SCIENCE/Dissertation/UTKFace/semantic_activation_morphoXAI_balancead_vf2.csv"
procesar_batch_semantic(test_loader, gradcam, scorecam, save_path=output_csv_balanceado, max_images=20000)

from matplotlib import cm

def visualizar_comparacion_semantic(image_tensor, label, gradcam, scorecam):
    resultado = detectar_landmarks_y_mascaras(image_tensor.cpu())
    if resultado is None:
        print("‚ùå No se detectaron landmarks.")
        return

    puntos, masks = resultado

    # Obtener CAMs
    cam_grad = gradcam.generate_cam(image_tensor)
    cam_score = scorecam.generate_cam(image_tensor)

    # Activaci√≥n por regi√≥n
    act_grad = cuantificar_activacion_soft(cam_grad, masks)
    act_score = cuantificar_activacion_soft(cam_score, masks)

    # Mapas sem√°nticos
    mapa_grad = generar_semantic_map(act_grad, masks)
    mapa_score = generar_semantic_map(act_score, masks)

    # Imagen original
    orig_img = image_tensor.detach().cpu().permute(1, 2, 0).numpy()
    orig_img = (orig_img * [0.229, 0.224, 0.225]) + [0.485, 0.456, 0.406]
    orig_img = np.clip(orig_img, 0, 1) ** 0.8

    # Heatmaps
    cmap = cm.get_cmap('jet')
    overlay_grad = np.clip(0.5 * orig_img + 0.5 * cmap(mapa_grad)[..., :3], 0, 1)
    overlay_score = np.clip(0.5 * orig_img + 0.5 * cmap(mapa_score)[..., :3], 0, 1)

    # Mostrar
    plt.figure(figsize=(10, 5))
    plt.subplot(1, 2, 1)
    plt.imshow(overlay_grad)
    plt.title("Semantic Map - Grad-CAM")
    plt.axis("off")

    plt.subplot(1, 2, 2)
    plt.imshow(overlay_score)
    plt.title("Semantic Map - Score-CAM")
    plt.axis("off")

    clase = "Mujer" if label == 1 else "Hombre"
    plt.suptitle(f"Clase verdadera: {clase}", fontsize=14)
    plt.tight_layout()
    plt.show()

# ‚úÖ Extraer un solo ejemplo del test_loader (batch de 64 ‚Üí tomar el primero)
sample_batch = next(iter(test_loader))
image = sample_batch[0][0].to(device)  # Primera imagen del batch
label = int(sample_batch[1][0].item()) # Primera etiqueta (0=Male, 1=Female)

# ‚úÖ Visualizar activaciones sem√°nticas
visualizar_comparacion_semantic(image, label, gradcam, scorecam)

def visualizar_mascaras_superpuestas_con_etiquetas(image_tensor, masks):
    # Imagen original
    orig_img = image_tensor.detach().cpu().permute(1, 2, 0).numpy()
    orig_img = (orig_img * [0.229, 0.224, 0.225]) + [0.485, 0.456, 0.406]
    orig_img = np.clip(orig_img, 0, 1)

    overlay = np.copy(orig_img)
    color_map = plt.cm.get_cmap('hsv', len(masks))

    fig, ax = plt.subplots(figsize=(6, 6))
    ax.imshow(overlay)

    for i, (region, mask) in enumerate(masks.items()):
        color = color_map(i)[:3]
        mask_rgb = np.stack([mask * c for c in color], axis=-1)
        overlay = np.clip(overlay + mask_rgb * 0.5, 0, 1)

        # Etiquetar regi√≥n (en centroide de m√°scara)
        ys, xs = np.where(mask > 0.2)
        if len(xs) > 0:
            x_mean = int(np.mean(xs))
            y_mean = int(np.mean(ys))
            ax.text(x_mean, y_mean, region.replace("_", "\n"), fontsize=7,
                    color='white', ha='center', va='center',
                    bbox=dict(boxstyle='round,pad=0.2', fc='black', alpha=0.5))

    ax.imshow(overlay)
    ax.set_title("Anatomical overlay masks with labels")
    ax.axis("off")
    plt.tight_layout()
    plt.show()

# Ejecutar en una imagen del test_loader
sample_batch = next(iter(test_loader))  # ‚ö†Ô∏è test_loader est√° correctamente definido
image = sample_batch[0][0].to(device)   # Primera imagen del batch

resultado = detectar_landmarks_y_mascaras(image.cpu())
if resultado is not None:
    puntos, masks = resultado
    visualizar_mascaras_superpuestas_con_etiquetas(image, masks)
else:
    print("‚ùå No se detectaron landmarks.")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Cargar archivo CSV generado
df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/MSC HEALTH DATA SCIENCE/Dissertation/UTKFace/semantic_activation_morphoXAI_balancead_vf2.csv")

# Mapear etiqueta binaria a g√©nero
df["Gender"] = df["label"].map({0: "Male", 1: "Female"})

# Extraer columnas de activaci√≥n
grad_cols = [c for c in df.columns if c.endswith("_grad")]
score_cols = [c for c in df.columns if c.endswith("_score")]

# Promedio por grupo de edad y g√©nero
df_grad = df.groupby(["age_group", "Gender"])[grad_cols].mean().reset_index()
df_score = df.groupby(["age_group", "Gender"])[score_cols].mean().reset_index()

# Formato largo para heatmap
df_grad_melted = df_grad.melt(id_vars=["age_group", "Gender"], var_name="Region", value_name="Activaci√≥n")
df_score_melted = df_score.melt(id_vars=["age_group", "Gender"], var_name="Region", value_name="Activaci√≥n")

df_grad_melted["Region"] = df_grad_melted["Region"].str.replace("_grad", "")
df_score_melted["Region"] = df_score_melted["Region"].str.replace("_score", "")

from scipy.stats import mannwhitneyu

def test_mannwhitney_por_region(df, columnas_score, columnas_grad):
    resultados = []

    for grad_col, score_col in zip(columnas_grad, columnas_score):
        region = grad_col.replace("_grad", "")
        hombres_g = df[df["Gender"] == "Male"][grad_col]
        mujeres_g = df[df["Gender"] == "Female"][grad_col]
        stat_g, p_g = mannwhitneyu(hombres_g, mujeres_g, alternative="two-sided")

        hombres_s = df[df["Gender"] == "Male"][score_col]
        mujeres_s = df[df["Gender"] == "Female"][score_col]
        stat_s, p_s = mannwhitneyu(hombres_s, mujeres_s, alternative="two-sided")

        resultados.append({
            "Region": region,
            "p-valuw Grad-CAM": p_g,
            "Significant Grad-CAM": "‚úÖ" if p_g < 0.05 else "‚Äî",
            "p-value Score-CAM": p_s,
            "Significant Score-CAM": "‚úÖ" if p_s < 0.05 else "‚Äî"
        })

    df_test = pd.DataFrame(resultados).sort_values("p-value Score-CAM")
    return df_test

# Ejecutar
df_test_mwu = test_mannwhitney_por_region(df, score_cols, grad_cols)
display(df_test_mwu)

import statsmodels.api as sm
from sklearn.preprocessing import StandardScaler

def calcular_odds_ratios(X_cols, df, label_col="label", nombre="Score-CAM"):
    X = df[X_cols]
    y = df[label_col]

    # Escalar
    scaler = StandardScaler()
    X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)

    # Modelo
    X_sm = sm.add_constant(X_scaled)
    model = sm.Logit(y, X_sm).fit(disp=0)

    # Resultados
    resultados = pd.DataFrame({
        "Region": X.columns,
        "Odds Ratio": np.exp(model.params[1:]),
        "p-value": model.pvalues[1:],
        "CI 2.5%": np.exp(model.conf_int().iloc[1:, 0]),
        "CI 97.5%": np.exp(model.conf_int().iloc[1:, 1]),
        "Method": nombre
    })

    return resultados.sort_values("Odds Ratio", ascending=False)

# Ejecutar para ambas t√©cnicas
odds_score = calcular_odds_ratios(score_cols, df, nombre="Score-CAM")
odds_grad = calcular_odds_ratios(grad_cols, df, nombre="Grad-CAM")

# Combinar para comparar
odds_comparado = pd.concat([odds_score, odds_grad], axis=0)
display(odds_comparado)

df_box = []
for region in score_cols:
    region_name = region.replace("_score", "")
    for _, row in df.iterrows():
        df_box.append({
            "Region": region_name,
            "Activation": row[region],
            "Gender": row["Gender"],
            "AgeGroup": row["age_group"]
        })

df_box = pd.DataFrame(df_box)

import matplotlib.pyplot as plt
import seaborn as sns

# üîÅ Aseg√∫rate de eliminar la regi√≥n "other"
df_box = df_box[df_box["Region"] != "other"]

# ‚úÖ Extraer y ordenar nombres de regiones
regiones_validas = sorted([col.replace("_score", "") for col in score_cols if "other" not in col])
df_box["Region"] = pd.Categorical(df_box["Region"], categories=regiones_validas, ordered=True)

# ‚úÖ Orden deseado para grupos de edad
orden_grupos = ["Teen", "Young adult", "Middle-aged", "Senior"]

# üé® Estilo visual
sns.set(style="whitegrid", font_scale=1.2)

# üìä Visualizaci√≥n
g = sns.catplot(
    data=df_box,
    kind="box",
    x="Region", y="Activation",
    hue="Gender", col="AgeGroup",
    col_order=orden_grupos,
    col_wrap=2,
    height=5, aspect=1.6,
    palette="Set2",
    showfliers=True,
    linewidth=1.0
)

g.set_titles("{col_name}")
g.set_axis_labels("Region", "Activation Ratio")

# üîÅ Rotar etiquetas del eje X correctamente
for ax in g.axes.flat:
    for label in ax.get_xticklabels():
        label.set_rotation(45)
        label.set_horizontalalignment('right')
    ax.set_ylim(0, 0.12)

# Crear df_box_grad con los mismos registros pero activaciones de Grad-CAM
df_box_grad = []
for region in grad_cols:
    region_name = region.replace("_grad", "")
    if region_name == "other":
        continue
    for _, row in df.iterrows():
        df_box_grad.append({
            "Region": region_name,
            "Activation": row[region],
            "Gender": row["Gender"],
            "AgeGroup": row["age_group"]
        })

df_box_grad = pd.DataFrame(df_box_grad)

# Orden expl√≠cito para regiones y grupos
regiones_validas = sorted([col.replace("_grad", "") for col in grad_cols if "other" not in col])
df_box_grad["Region"] = pd.Categorical(df_box_grad["Region"], categories=regiones_validas, ordered=True)
orden_grupos = ["Teen", "Young adult", "Middle-aged", "Senior"]

# Estilo y paleta
sns.set(style="whitegrid", font_scale=1.2)

# Generar los subplots por grupo de edad
g = sns.catplot(
    data=df_box_grad,
    kind="box",
    x="Region", y="Activation",
    hue="Gender", col="AgeGroup",
    col_order=orden_grupos,
    col_wrap=2,
    height=5, aspect=1.6,
    palette="Set2",
    showfliers=True,
    linewidth=1.0
)

g.set_titles("{col_name}")
g.set_axis_labels("Region", "Activation Ratio")

# Rotar etiquetas del eje X y fijar l√≠mites
for ax in g.axes.flat:
    for label in ax.get_xticklabels():
        label.set_rotation(45)
        label.set_horizontalalignment('right')
    ax.set_ylim(0, 0.12)

# üîÅ Preparar df_box_score con activaciones Score-CAM
df_box_score = df_box.copy()
df_box_score["Method"] = "Score-CAM"
df_box_score["Activation"] = df_box_score.apply(
    lambda row: df.loc[
        (df["Gender"] == row["Gender"]) &
        (df["age_group"] == row["AgeGroup"]),
        f"{row['Region']}_score"
    ].values[0],
    axis=1
)

# üîÅ Preparar df_box_grad con activaciones Grad-CAM
df_box_grad = df_box.copy()
df_box_grad["Method"] = "Grad-CAM"
df_box_grad["Activation"] = df_box_grad.apply(
    lambda row: df.loc[
        (df["Gender"] == row["Gender"]) &
        (df["age_group"] == row["AgeGroup"]),
        f"{row['Region']}_grad"
    ].values[0],
    axis=1
)

# üîó Unir en un √∫nico DataFrame
df_box_combined = pd.concat([df_box_score, df_box_grad], ignore_index=True)

import statsmodels.formula.api as smf

def anova_interaccion(df_box, metodo):
    resultados = []
    for region in df_box["Region"].unique():
        sub = df_box[(df_box["Region"] == region) & (df_box["Method"] == metodo)]
        if sub["Activation"].nunique() > 1:  # evitar errores por falta de variaci√≥n
            modelo = smf.ols("Activation ~ C(Gender) * C(AgeGroup)", data=sub).fit()
            tabla = sm.stats.anova_lm(modelo, typ=2)
            p_interaccion = tabla.loc["C(Gender):C(AgeGroup)", "PR(>F)"]
            resultados.append({
                "Region": region,
                "p-value Interaction": p_interaccion,
                "Significant": "‚úÖ" if p_interaccion < 0.05 else "‚Äî",
                "Method": metodo
            })
    return pd.DataFrame(resultados)

# Ejecutar ANOVA para ambos m√©todos
anova_score = anova_interaccion(df_box_combined, "Score-CAM")
anova_grad = anova_interaccion(df_box_combined, "Grad-CAM")
anova_total = pd.concat([anova_score, anova_grad], ignore_index=True)

# Resumen final: cu√°ntas regiones significativas por t√©cnica
resumen = (
    anova_total[anova_total["Significant"] == "‚úÖ"]
    .groupby("Method")
    .size()
    .reset_index(name="Regions with significant interaction")
)

print("Regions with significant interaction by method:")
display(resumen)

print("\n Full detail ANOVA by region:")
display(anova_total.sort_values(["Method", "p-value Interaction"]))

# Crear tabla media por grupo de edad y g√©nero
resumen_regiones = []

for region in score_cols:
    nombre_region = region.replace("_score", "")
    if nombre_region == "other":
        continue
    for grupo in df["age_group"].unique():
        hombres = df[(df["age_group"] == grupo) & (df["Gender"] == "Male")][region]
        mujeres = df[(df["age_group"] == grupo) & (df["Gender"] == "Female")][region]

        media_h = hombres.mean()
        media_m = mujeres.mean()
        diferencia = media_m - media_h

        resumen_regiones.append({
            "Age Group": grupo,
            "Region": nombre_region,
            "Male Average": media_h,
            "Female Average": media_m,
            "Difference (F - M)": diferencia,
            "Absolute Difference": abs(diferencia)
        })

df_regiones_score = pd.DataFrame(resumen_regiones)

# Top 3 regiones con mayor diferencia absoluta por grupo
top3_por_grupo = (
    df_regiones_score
    .sort_values(["Age Group", "Absolute Difference"], ascending=[True, False])
    .groupby("Age Group")
    .head(3)
)

display(top3_por_grupo)

def top3_regiones_por_grupo(df, columnas, sufijo):
    resumen = []

    for col in columnas:
        region = col.replace(f"_{sufijo}", "")
        if region == "other":
            continue
        for grupo in df["age_group"].unique():
            hombres = df[(df["age_group"] == grupo) & (df["Gender"] == "Male")][col]
            mujeres = df[(df["age_group"] == grupo) & (df["Gender"] == "Female")][col]

            media_h = hombres.mean()
            media_m = mujeres.mean()
            diferencia = media_m - media_h

            resumen.append({
            "Age Group": grupo,
            "Region": region,
            "Male Average": media_h,
            "Female Average": media_m,
            "Difference (F - M)": diferencia,
            "Absolute Difference": abs(diferencia),
            "Method": f"{sufijo.upper()}"
        })

    df_out = pd.DataFrame(resumen)
    top3 = df_out.sort_values(["Age Group", "Absolute Difference"], ascending=[True, False]) \
                 .groupby(["Age Group", "Method"]).head(3)
    return top3

# Ejecutar para ambas t√©cnicas
top3_score = top3_regiones_por_grupo(df, score_cols, "score")
top3_grad = top3_regiones_por_grupo(df, grad_cols, "grad")

# Combinar
top3_total = pd.concat([top3_score, top3_grad])
display(top3_total.sort_values(["Age Group", "Method"]))

# Generar resumen de frases autom√°ticas
def generar_resumen(df_top3):
    frases = []
    for grupo in df_top3["Age Group"].unique():
        for metodo in df_top3["Method"].unique():
            subset = df_top3[(df_top3["Age Group"] == grupo) & (df_top3["Method"] == metodo)]
            if subset.empty:
                continue
            regiones = subset["Region"].tolist()
            frases.append(
                f"In the group '{grupo}' ({metodo}), the most discriminating regions between men and women were: {', '.join(regiones)}."
            )
    return frases

resumen_frases = generar_resumen(top3_total)
for frase in resumen_frases:
    print(frase)

import matplotlib.pyplot as plt
import seaborn as sns

# Estilo
sns.set(style="whitegrid", font_scale=1.1)

# Crear gr√°fico
g = sns.catplot(
    data=top3_total,
    kind="bar",
    x="Region", y="Absolute Difference",
    hue="Method", col="Age Group",
    col_order=["Teen", "Young adult", "Middle-aged", "Senior"],
    palette="Set2",
    height=4.5, aspect=1.2
)

g.set_titles("{col_name}")
g.set_axis_labels("Region", "|Difference F-M|")
g._legend.set_title("XAI")

# Rotar etiquetas
for ax in g.axes.flat:
    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')

g.fig.suptitle("Top 3 most discriminating regions by age group and technique XAI", fontsize=16, y=1.05)
plt.tight_layout()
plt.show()

# Funci√≥n para obtener predicci√≥n binaria de g√©nero (0 = Male, 1 = Female)
def predecir_clase(model, image_tensor):
    with torch.no_grad():
        output = model(image_tensor.unsqueeze(0).to(device))
        prob = torch.sigmoid(output).item()
    return 1 if prob >= 0.5 else 0

# Recorrer val_loader y clasificar en aciertos/errores
activaciones_score = []
max_images = 5000
count = 0

for batch in test_loader:
    image, label, age, group_age, filename = batch
    image = image[0].to(device)
    label = int(label[0].item())
    group_age = group_age[0]
    filename = filename[0]

    # Predicci√≥n
    pred = predecir_clase(vgg_gender, image)
    correcto = int(pred == label)

    resultado = detectar_landmarks_y_mascaras(image.cpu())
    if resultado is None:
        continue
    puntos, masks = resultado

    # CAM Score
    cam_score = scorecam.generate_cam(image)
    act_score = cuantificar_activacion_soft(cam_score, masks)

    fila = {
        "label": label,
        "pred": pred,
        "correcto": correcto,
        "grupo_edad": group_age,
        "filename": filename
    }

    for region, valor in act_score.items():
        fila[region] = valor

    activaciones_score.append(fila)
    count += 1
    if count >= max_images:
        break

df_score_acc = pd.DataFrame(activaciones_score)

regiones_validas = [col for col in df_score_acc.columns if col not in ["label", "pred", "correcto", "grupo_edad", "filename"]]

# Calcular medias
media_correctos = df_score_acc[df_score_acc["correcto"] == 1][regiones_validas].mean()
media_errores = df_score_acc[df_score_acc["correcto"] == 0][regiones_validas].mean()

df_diff = pd.DataFrame({
    "Region": regiones_validas,
    "Correctness Average": media_correctos.values,
    "Incorrectness Average": media_errores.values,
    "Difference (Correct - Incorrect)": media_correctos.values - media_errores.values
}).sort_values("Difference (Correct - Incorrect)", ascending=False)

display(df_diff)

plt.figure(figsize=(12, 6))
sns.barplot(data=df_diff, x="Region", y="Difference (Correct - Incorrect)", palette="coolwarm")
plt.axhline(0, color="gray", linestyle="--")
plt.xticks(rotation=45, ha='right')
plt.title("Activation Difference by Region: Correct vs. Incorrect (Score-CAM)")
plt.ylabel("Œî Average activation (Correct - Incorrect)")
plt.tight_layout()
plt.show()

activaciones_grad = []
count = 0
max_images = 5000

for batch in test_loader:
    image, label, age, group_age, filename = batch
    image = image[0].to(device)
    label = int(label[0].item())
    group_age = group_age[0]
    filename = filename[0]

    # Predicci√≥n
    pred = predecir_clase(vgg_gender, image)
    correcto = int(pred == label)

    resultado = detectar_landmarks_y_mascaras(image.cpu())
    if resultado is None:
        continue
    puntos, masks = resultado

    # CAM Grad
    cam_grad = gradcam.generate_cam(image)
    act_grad = cuantificar_activacion_soft(cam_grad, masks)

    fila = {
        "label": label,
        "pred": pred,
        "correcto": correcto,
        "grupo_edad": group_age,
        "filename": filename
    }

    for region, valor in act_grad.items():
        fila[region] = valor

    activaciones_grad.append(fila)
    count += 1
    if count >= max_images:
        break

df_grad_acc = pd.DataFrame(activaciones_grad)

regiones_validas = [col for col in df_grad_acc.columns if col not in ["label", "pred", "correcto", "grupo_edad", "filename"]]

media_correctos = df_grad_acc[df_grad_acc["correcto"] == 1][regiones_validas].mean()
media_errores = df_grad_acc[df_grad_acc["correcto"] == 0][regiones_validas].mean()

df_diff_grad = pd.DataFrame({
    "Region": regiones_validas,
    "Correctness Average": media_correctos.values,
    "Incorrectness Average": media_errores.values,
    "Difference (Correct - Incorrect)": media_correctos.values - media_errores.values
}).sort_values("Difference (Correct - Incorrect)", ascending=False)

display(df_diff_grad)

plt.figure(figsize=(12, 6))
sns.barplot(data=df_diff_grad, x="Region", y="Difference (Correct - Incorrect)", palette="coolwarm")
plt.axhline(0, color="gray", linestyle="--")
plt.xticks(rotation=45, ha='right')
plt.title("Difference in activation by region: Correct vs. Incorrect (Grad-CAM)")
plt.ylabel("Œî Average activation (Correct - Incorrect)")
plt.tight_layout()
plt.show()

# Agregamos columna de m√©todo
df_diff_score = df_diff.copy()
df_diff_score["Method"] = "Score-CAM"

df_diff_grad = df_diff_grad.copy()
df_diff_grad["Method"] = "Grad-CAM"

# Combinar ambos
df_comparado = pd.concat([df_diff_score, df_diff_grad], ignore_index=True)

plt.figure(figsize=(14, 6))
sns.barplot(data=df_comparado,
            x="Region", y="Difference (Correct - Incorrect)",
            hue="Method", palette="Set2")

plt.axhline(0, color="gray", linestyle="--")
plt.xticks(rotation=45, ha='right')
plt.ylabel("Œî Average activation (Correct - Incorrect)")
plt.title("Grad-CAM vs Score-CAM Comparison by Region: Correct vs Incorrect")
plt.grid(True)
plt.tight_layout()
plt.show()

from collections import defaultdict

# Diccionario para almacenar los mapas por grupo
mapas_por_grupo = defaultdict(list)

# Procesar el val_loader_balanceado
for batch in test_loader:
    image, label, age, group_age, filename = batch
    image = image[0].to(device)
    label = int(label[0].item())
    grupo = group_age[0]
    clase = "Female" if label == 1 else "Male"

    resultado = detectar_landmarks_y_mascaras(image.cpu())
    if resultado is None:
        continue
    _, masks = resultado

    cam_score = scorecam.generate_cam(image)
    act_score = cuantificar_activacion_soft(cam_score, masks)
    mapa_score = generar_semantic_map(act_score, masks)

    mapas_por_grupo[(clase, grupo)].append(mapa_score)

# Mostrar promedio por grupo
import matplotlib.pyplot as plt

grupos_orden = ["Teen", "Young adult", "Middle-aged", "Senior"]
clases_orden = ["Male", "Female"]

fig, axs = plt.subplots(len(clases_orden), len(grupos_orden), figsize=(16, 6))

for i, clase in enumerate(clases_orden):
    for j, grupo in enumerate(grupos_orden):
        key = (clase, grupo)
        mapas = mapas_por_grupo.get(key, [])
        if mapas:
            promedio = np.mean(np.stack(mapas), axis=0)
            axs[i, j].imshow(promedio, cmap="jet")
            axs[i, j].set_title(f"{clase} - {grupo}")
        else:
            axs[i, j].text(0.5, 0.5, "Sin datos", ha="center")
        axs[i, j].axis("off")

plt.suptitle("Average Score-CAM maps by class and age group", fontsize=16)
plt.tight_layout()
plt.show()

mapas_grad_por_grupo = defaultdict(list)

for batch in test_loader:
    image, label, age, group_age, filename = batch
    image = image[0].to(device)
    label = int(label[0].item())
    grupo = group_age[0]
    clase = "Female" if label == 1 else "Male"

    resultado = detectar_landmarks_y_mascaras(image.cpu())
    if resultado is None:
        continue
    _, masks = resultado

    cam_grad = gradcam.generate_cam(image)
    act_grad = cuantificar_activacion_soft(cam_grad, masks)
    mapa_grad = generar_semantic_map(act_grad, masks)

    mapas_grad_por_grupo[(clase, grupo)].append(mapa_grad)

fig, axs = plt.subplots(len(clases_orden), len(grupos_orden), figsize=(16, 6))

for i, clase in enumerate(clases_orden):
    for j, grupo in enumerate(grupos_orden):
        key = (clase, grupo)
        mapas = mapas_grad_por_grupo.get(key, [])
        if mapas:
            promedio = np.mean(np.stack(mapas), axis=0)
            axs[i, j].imshow(promedio, cmap="jet")
            axs[i, j].set_title(f"{clase} - {grupo}")
        else:
            axs[i, j].text(0.5, 0.5, "Sin datos", ha="center")
        axs[i, j].axis("off")

plt.suptitle("Grad-CAM average maps by class and age group", fontsize=16)
plt.tight_layout()
plt.show()

import cv2
from scipy.stats import pearsonr

def analizar_robustez_blur(image_tensor, label, cam_func, metodo="Score-CAM"):
    # Imagen original
    resultado = detectar_landmarks_y_mascaras(image_tensor.cpu())
    if resultado is None:
        print("‚ùå No se detectaron landmarks.")
        return
    _, masks = resultado

    # CAM original
    cam_original = cam_func.generate_cam(image_tensor)
    act_orig = cuantificar_activacion_soft(cam_original, masks)
    mapa_orig = generar_semantic_map(act_orig, masks)

    # Aplicar desenfoque
    image_np = image_tensor.permute(1, 2, 0).cpu().numpy()
    image_np = (image_np * [0.229, 0.224, 0.225]) + [0.485, 0.456, 0.406]
    image_np = np.clip(image_np, 0, 1)
    image_blur = cv2.GaussianBlur((image_np * 255).astype(np.uint8), (9, 9), 5)

    # Volver a tensor
    image_blur_tensor = transforms.ToTensor()(Image.fromarray(image_blur)).to(device)

    # CAM con imagen desenfocada
    cam_blur = cam_func.generate_cam(image_blur_tensor)
    act_blur = cuantificar_activacion_soft(cam_blur, masks)
    mapa_blur = generar_semantic_map(act_blur, masks)

    # Correlaci√≥n entre activaciones
    regiones = list(act_orig.keys())
    orig_vals = [act_orig[r] for r in regiones]
    blur_vals = [act_blur[r] for r in regiones]
    correlacion, _ = pearsonr(orig_vals, blur_vals)

    # Visualizaci√≥n
    fig, axs = plt.subplots(1, 2, figsize=(10, 4))
    axs[0].imshow(mapa_orig, cmap="jet")
    axs[0].set_title("Original map")

    axs[1].imshow(mapa_blur, cmap="jet")
    axs[1].set_title("Map with blur")
    for ax in axs: ax.axis("off")
    plt.suptitle(f"{metodo} - Correlation activations: {correlacion:.2f}", fontsize=14)
    plt.tight_layout()
    plt.show()

    return correlacion

# Tomar una imagen
sample_batch = next(iter(test_loader))
image = sample_batch[0][0].to(device)
label = int(sample_batch[1][0].item())

# Ejecutar an√°lisis para Score-CAM
cor_score = analizar_robustez_blur(image, label, scorecam, metodo="Score-CAM")

# Ejecutar an√°lisis para Grad-CAM
cor_grad = analizar_robustez_blur(image, label, gradcam, metodo="Grad-CAM")

def evaluar_robustez_lote(val_loader, cam_func, metodo="Score-CAM", max_images=100):
    correlaciones = []
    count = 0

    for batch in val_loader:
        image, label, *_ = batch
        image = image[0].to(device)
        label = int(label[0].item())

        resultado = detectar_landmarks_y_mascaras(image.cpu())
        if resultado is None:
            continue
        _, masks = resultado

        try:
            # Imagen original
            cam_orig = cam_func.generate_cam(image)
            act_orig = cuantificar_activacion_soft(cam_orig, masks)

            # Imagen desenfocada
            image_np = image.permute(1, 2, 0).cpu().numpy()
            image_np = (image_np * [0.229, 0.224, 0.225]) + [0.485, 0.456, 0.406]
            image_np = np.clip(image_np, 0, 1)
            image_blur = cv2.GaussianBlur((image_np * 255).astype(np.uint8), (9, 9), 5)
            image_blur_tensor = transforms.ToTensor()(Image.fromarray(image_blur)).to(device)

            cam_blur = cam_func.generate_cam(image_blur_tensor)
            act_blur = cuantificar_activacion_soft(cam_blur, masks)

            regiones = list(act_orig.keys())
            vals_orig = [act_orig[r] for r in regiones]
            vals_blur = [act_blur[r] for r in regiones]

            corr, _ = pearsonr(vals_orig, vals_blur)
            correlaciones.append(corr)
            count += 1

        except Exception as e:
            print(f"Error en imagen {count}: {e}")
            continue

        if count >= max_images:
            break

    return correlaciones

corrs_score = evaluar_robustez_lote(test_loader, scorecam, metodo="Score-CAM", max_images=1000)
corrs_grad = evaluar_robustez_lote(test_loader, gradcam, metodo="Grad-CAM", max_images=1000)

print(f"Average Score-CAM correlation (n={len(corrs_score)}): {np.mean(corrs_score):.3f}")
print(f"Grad-CAM average correlation  (n={len(corrs_grad)}): {np.mean(corrs_grad):.3f}")

# Crear DataFrame para visualizaci√≥n
df_corr = pd.DataFrame({
    "Correlation": corrs_score + corrs_grad,
    "Method": ["Score-CAM"] * len(corrs_score) + ["Grad-CAM"] * len(corrs_grad)
})

import seaborn as sns
import matplotlib.pyplot as plt

sns.set(style="whitegrid", font_scale=1.2)

plt.figure(figsize=(8, 5))
sns.boxplot(data=df_corr, x="Method", y="Correlation", palette="Set2")
sns.swarmplot(data=df_corr, x="Method", y="Correlation", color=".3", alpha=0.5)

plt.title("Robustness of the XAI to blur: Correlation of activations")
plt.ylim(0, 1.05)
plt.tight_layout()
plt.grid(True)
plt.show()

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Extraer vectores por imagen
regiones_sem = [col for col in df.columns if col.endswith("_score") and "other" not in col]
X = df[regiones_sem].copy()
y_genero = df["Gender"]
y_edad = df["age_group"]

# Escalar datos
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

print(f"Explained variance: {pca.explained_variance_ratio_.sum():.2f}")

plt.figure(figsize=(7, 5))
sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y_genero, palette="Set2", s=60, alpha=0.8)
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.title("PCA on Score-CAM Activations - By Gender")
plt.grid(True)
plt.tight_layout()
plt.show()

plt.figure(figsize=(7, 5))
sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y_edad, palette="cool", s=60, alpha=0.8)
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.title("PCA on Score-CAM activations - By Age Group")
plt.grid(True)
plt.tight_layout()
plt.show()

# Extraer columnas de activaci√≥n Grad-CAM
regiones_grad = [col for col in df.columns if col.endswith("_grad") and "other" not in col]
X_grad = df[regiones_grad].copy()
y_genero = df["Gender"]
y_edad = df["age_group"]

# Escalar
X_grad_scaled = scaler.fit_transform(X_grad)

pca_grad = PCA(n_components=2)
X_pca_grad = pca_grad.fit_transform(X_grad_scaled)

print(f"Explained variance (Grad-CAM): {pca_grad.explained_variance_ratio_.sum():.2f}")

plt.figure(figsize=(7, 5))
sns.scatterplot(x=X_pca_grad[:, 0], y=X_pca_grad[:, 1], hue=y_genero, palette="Set2", s=60, alpha=0.8)
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.title("PCA on Grad-CAM activations - By Gender")
plt.grid(True)
plt.tight_layout()
plt.show()

plt.figure(figsize=(7, 5))
sns.scatterplot(x=X_pca_grad[:, 0], y=X_pca_grad[:, 1], hue=y_edad, palette="cool", s=60, alpha=0.8)
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.title("PCA on Grad-CAM activations - By Age Group")
plt.grid(True)
plt.tight_layout()
plt.show()

from sklearn.manifold import TSNE

# Aplicar t-SNE a activaciones Score-CAM
tsne = TSNE(n_components=2, perplexity=30, learning_rate=200, random_state=42)
X_tsne_score = tsne.fit_transform(X_scaled)

# Visualizar por g√©nero
plt.figure(figsize=(7, 5))
sns.scatterplot(x=X_tsne_score[:, 0], y=X_tsne_score[:, 1], hue=y_genero, palette="Set2", s=60, alpha=0.8)
plt.title("t-SNE on Score-CAM activations - Gender")
plt.xticks([])
plt.yticks([])
plt.grid(True)
plt.tight_layout()
plt.show()

# Visualizar por grupo de edad
plt.figure(figsize=(7, 5))
sns.scatterplot(x=X_tsne_score[:, 0], y=X_tsne_score[:, 1], hue=y_edad, palette="cool", s=60, alpha=0.8)
plt.title("t-SNE on Score-CAM activations - Age Group")
plt.xticks([])
plt.yticks([])
plt.grid(True)
plt.tight_layout()
plt.show()

# Aplicar t-SNE a activaciones Grad-CAM
tsne_grad = TSNE(n_components=2, perplexity=30, learning_rate=200, random_state=42)
X_tsne_grad = tsne_grad.fit_transform(X_grad_scaled)

# Visualizar por g√©nero
plt.figure(figsize=(7, 5))
sns.scatterplot(x=X_tsne_grad[:, 0], y=X_tsne_grad[:, 1], hue=y_genero, palette="Set2", s=60, alpha=0.8)
plt.title("t-SNE on Grad-CAM activations - Gender")
plt.xticks([])
plt.yticks([])
plt.grid(True)
plt.tight_layout()
plt.show()

# Visualizar por grupo de edad
plt.figure(figsize=(7, 5))
sns.scatterplot(x=X_tsne_grad[:, 0], y=X_tsne_grad[:, 1], hue=y_edad, palette="cool", s=60, alpha=0.8)
plt.title("t-SNE on Grad-CAM activations - Age Group")
plt.xticks([])
plt.yticks([])
plt.grid(True)
plt.tight_layout()
plt.show()

